{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkLM6VTE5YtA"
      },
      "source": [
        "# Comparing time series predictions of COVID-19 deaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkrYuu3g5YtC"
      },
      "source": [
        "\n",
        "![Comparison](comparison.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w71E-Yy5YtD"
      },
      "source": [
        "# Task 1: Understand the basics of time series analysis\n",
        "\n",
        "- What's a time series?\n",
        "  - It's a table of values, such as temperature or stock price, that are observed at regular times, such as every hour or every day.\n",
        "  - Here is a simple time series: temperatures in Celcius in The Neighborhood over 10 days.\n",
        "\n",
        "| Day         | Temperature |\n",
        "| ----------- | ----------- |\n",
        "| 11 May 2022 | 23          |\n",
        "| 12 May 2022 | 21          |\n",
        "| 13 May 2022 | 24          |\n",
        "| 14 May 2022 | 19          |\n",
        "| 15 May 2022 | 18          |\n",
        "| 16 May 2022 | 20          |\n",
        "| 17 May 2022 | 21          |\n",
        "| 18 May 2022 | 22          |\n",
        "| 19 May 2022 | 20          |\n",
        "| 20 May 2022 | 21          |\n",
        "\n",
        "- One would want to know: what's the temperature after 20 May 2022? Predicting the data values (temperature) in the future is called forecasting.\n",
        "- Workflow for time series analysis:\n",
        "  - Step 1: Underst the data (Task 2)\n",
        "  - Step 2: Identify the right model (Tasks 3, 4, 5, 6, 7)\n",
        "  - Step 3: Use the best model to forecast future values (Task 8)\n",
        "\n",
        "- To make the best out of this hands-on project, we need to learn a few concepts before the hands-on tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Z6LP8q5YtD"
      },
      "source": [
        "## Common features in time series data\n",
        "\n",
        "\n",
        "![Trends and seasons](trends_seasons.png)\n",
        "\n",
        "- These are three common features in every time series data:\n",
        "    - Seasons: The regular ups and downs in your data, like in the figure above.\n",
        "    - Trends: When you find that data is general going upward or downword. In the figure above, there is an upward trend.\n",
        "    - Cycles: Where there are ups and downs in the data that do not seem to repeat regularly.\n",
        "- We can observe the above three features by plotting the data, and checking it visually.\n",
        "\n",
        "## Stationarity\n",
        "\n",
        "- A stationary time series: it's the time series in which the average and variance do not change; so these statistical properties are *independent of time*.\n",
        "- Therefore, a time series with trends is not stationary. An example of such data is in the figure above.\n",
        "- Non-stationary data are not easy to predict, but stationary data are much easier to predict.\n",
        "- Therefore, we need to make our time series data stationary before we feed them into statistical models.\n",
        "- You will learn how to turn non-stationary data into stationary data by using differencing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OcwBO0e5YtE"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "We obtain the dataset from the Github page of John Hopkins Center for System Science and Engineering:\n",
        "\n",
        "[https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUJKENHM5YtE"
      },
      "source": [
        "# Task 2: Explore and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pmdarima\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VFg9ZvL9MtF",
        "outputId": "1c160bc2-db8a-4700-9bc9-1dbadf9ae157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.3.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install pystan==2.19.1.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "AlStTjYc-HPN",
        "outputId": "a062b800-0a06-486e-ee8c-a27086a8b1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-24.0 setuptools-69.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pystan==2.19.1.1\n",
            "  Downloading pystan-2.19.1.1.tar.gz (16.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython!=0.25.1,>=0.22 in /usr/local/lib/python3.10/dist-packages (from pystan==2.19.1.1) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pystan==2.19.1.1) (1.23.5)\n",
            "Building wheels for collected packages: pystan\n",
            "  Building wheel for pystan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pystan: filename=pystan-2.19.1.1-cp310-cp310-linux_x86_64.whl size=61975554 sha256=62a2b819c94a0f9e849b0af7fdd179244657caf05c9a0bc2ff5c207823cf4814\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/1c/94/4516243362eedbedad15ac4389691ee3bf2d45bec2639c9d8b\n",
            "Successfully built pystan\n",
            "Installing collected packages: pystan\n",
            "Successfully installed pystan-2.19.1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prophet\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ze7O0xa9k7B",
        "outputId": "4cb5d474-d7a4-479c-db2a-f9465b99bd90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: prophet in /usr/local/lib/python3.10/dist-packages (1.1.5)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from prophet) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.5.3)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.41)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from prophet) (4.66.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet) (6.1.1)\n",
            "Requirement already satisfied: stanio~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.3.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays>=0.25->prophet) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.4->prophet) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->holidays>=0.25->prophet) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejbKY5k35YtE"
      },
      "outputs": [],
      "source": [
        "import pmdarima as pm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from prophet import Prophet\n",
        "from prophet.plot import plot_plotly, add_changepoints_to_plot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydzJ79B65YtF"
      },
      "source": [
        "First, get the data for the daily deaths in all countries in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJUmb3gr5YtF"
      },
      "source": [
        "Let's have a look at how the data looks like, by using the head() method from the DataFrame class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "Jx7Kr3JC5YtG",
        "outputId": "890e2e05-e7c7-48fd-e654-7506bffe00f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Province/State Country/Region       Lat       Long  1/22/20  1/23/20  \\\n",
              "0            NaN    Afghanistan  33.93911  67.709953        0        0   \n",
              "1            NaN        Albania  41.15330  20.168300        0        0   \n",
              "2            NaN        Algeria  28.03390   1.659600        0        0   \n",
              "3            NaN        Andorra  42.50630   1.521800        0        0   \n",
              "4            NaN         Angola -11.20270  17.873900        0        0   \n",
              "\n",
              "   1/24/20  1/25/20  1/26/20  1/27/20  ...  5/6/22  5/7/22  5/8/22  5/9/22  \\\n",
              "0        0        0        0        0  ...    7684    7684    7684    7685   \n",
              "1        0        0        0        0  ...    3496    3496    3497    3497   \n",
              "2        0        0        0        0  ...    6875    6875    6875    6875   \n",
              "3        0        0        0        0  ...     153     153     153     153   \n",
              "4        0        0        0        0  ...    1900    1900    1900    1900   \n",
              "\n",
              "   5/10/22  5/11/22  5/12/22  5/13/22  5/14/22  5/15/22  \n",
              "0     7685     7686     7686     7686     7687     7690  \n",
              "1     3497     3497     3497     3497     3497     3497  \n",
              "2     6875     6875     6875     6875     6875     6875  \n",
              "3      153      153      153      153      153      153  \n",
              "4     1900     1900     1900     1900     1900     1900  \n",
              "\n",
              "[5 rows x 849 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e01205e6-01d6-4231-ba32-b2bd225e9f9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Province/State</th>\n",
              "      <th>Country/Region</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Long</th>\n",
              "      <th>1/22/20</th>\n",
              "      <th>1/23/20</th>\n",
              "      <th>1/24/20</th>\n",
              "      <th>1/25/20</th>\n",
              "      <th>1/26/20</th>\n",
              "      <th>1/27/20</th>\n",
              "      <th>...</th>\n",
              "      <th>5/6/22</th>\n",
              "      <th>5/7/22</th>\n",
              "      <th>5/8/22</th>\n",
              "      <th>5/9/22</th>\n",
              "      <th>5/10/22</th>\n",
              "      <th>5/11/22</th>\n",
              "      <th>5/12/22</th>\n",
              "      <th>5/13/22</th>\n",
              "      <th>5/14/22</th>\n",
              "      <th>5/15/22</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>33.93911</td>\n",
              "      <td>67.709953</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7684</td>\n",
              "      <td>7684</td>\n",
              "      <td>7684</td>\n",
              "      <td>7685</td>\n",
              "      <td>7685</td>\n",
              "      <td>7686</td>\n",
              "      <td>7686</td>\n",
              "      <td>7686</td>\n",
              "      <td>7687</td>\n",
              "      <td>7690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Albania</td>\n",
              "      <td>41.15330</td>\n",
              "      <td>20.168300</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3496</td>\n",
              "      <td>3496</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "      <td>3497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>28.03390</td>\n",
              "      <td>1.659600</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "      <td>6875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>42.50630</td>\n",
              "      <td>1.521800</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Angola</td>\n",
              "      <td>-11.20270</td>\n",
              "      <td>17.873900</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "      <td>1900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 849 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e01205e6-01d6-4231-ba32-b2bd225e9f9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e01205e6-01d6-4231-ba32-b2bd225e9f9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e01205e6-01d6-4231-ba32-b2bd225e9f9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5e7bd292-5020-4d26-bce5-5b9a1e1fd94f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e7bd292-5020-4d26-bce5-5b9a1e1fd94f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5e7bd292-5020-4d26-bce5-5b9a1e1fd94f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "deaths_df = pd.read_csv('/content/time_series_covid19_deaths_global.csv')\n",
        "deaths_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1dcCcns5YtG"
      },
      "source": [
        "And here are the columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E741udhW5YtG",
        "outputId": "67cc6091-ca68-478b-89e9-cdee26f27d6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Province/State', 'Country/Region', 'Lat', 'Long', '1/22/20', '1/23/20',\n",
              "       '1/24/20', '1/25/20', '1/26/20', '1/27/20',\n",
              "       ...\n",
              "       '5/6/22', '5/7/22', '5/8/22', '5/9/22', '5/10/22', '5/11/22', '5/12/22',\n",
              "       '5/13/22', '5/14/22', '5/15/22'],\n",
              "      dtype='object', length=849)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "deaths_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URBhs_s55YtG"
      },
      "source": [
        "So the columns are arranged as follows: <br>\n",
        "<ul>\n",
        "    <li>\n",
        "        feature columns such as Province, Country etc. </li>\n",
        "    <li>date columns, starting from the first reporting date, 1/22/2020, until two days ago (with respect to the time of recording this course</li>\n",
        "   </ul>\n",
        "\n",
        "The date columns start from 1/22/20, so let's take the data in these columns separately.\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tnK4L7O5YtH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfk92jp35YtH"
      },
      "source": [
        "Transpose the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m06ZLO7i5YtH"
      },
      "outputs": [],
      "source": [
        "d = d.transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS1T4haM5YtH"
      },
      "source": [
        "Then sum row-wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsZRI-Sd5YtH"
      },
      "outputs": [],
      "source": [
        "d = d.sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJvsQoaj5YtH"
      },
      "source": [
        "We only need the numeric values here, so we convert `d` to a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq6fGnme5YtH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rheu26wA5YtH"
      },
      "source": [
        "Create a new data frame with two columns. This will be our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG4PSird5YtH"
      },
      "outputs": [],
      "source": [
        "dataset = pd.DataFrame(columns=['ds', 'y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBzbTXOJ5YtI"
      },
      "source": [
        "Get the dates from the columns in `deaths_df` data frame, starting from the fifth column. They will be obtained in string format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utpeFhw-5YtI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIA-Uq995YtI"
      },
      "source": [
        "Convert the string dates into the datetime format using the `to_datetime()` method, so that we can perform datetime operations on them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0yZP53s5YtI"
      },
      "outputs": [],
      "source": [
        "dates = list(pd.to_datetime(dates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNSKju6k5YtI"
      },
      "source": [
        "Now, assign the dates and deaths data to the columns in the new dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovQv9lGa5YtI"
      },
      "outputs": [],
      "source": [
        "dataset['ds'] = dates\n",
        "dataset['y'] = d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bIovMU05YtI"
      },
      "source": [
        "We will need to have only 1 data column, y, with the index being `ds`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-V54e2M5YtI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoT84BCy5YtI"
      },
      "source": [
        "Let's plot the daily number of deaths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xhkyZ7x5YtJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(dataset)\n",
        "plt.savefig('cummulative_daily_deaths', bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOLkOdaJ5YtJ"
      },
      "source": [
        "The increase has been tremendous, but it seems to start to plateau. Maybe too early to know? I'd leave that discussion to the scientists in charge.<br>\n",
        "\n",
        "This time series is obvious non-stationary, and we cannot observe any seasonal behavior here. Let's perform seasonal decomposition of the data by using the `seasonal_decompose()` function from `statsmodel`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "431qRwFv5YtJ"
      },
      "outputs": [],
      "source": [
        "seas_d=sm.tsa.seasonal_decompose(dataset,model='add');\n",
        "fig=seas_d.plot()\n",
        "fig.set_figheight(4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blslq9WS5YtJ"
      },
      "source": [
        "The data is both seasonal and trending upwords. We need the data to be stationary so that we can apply our models to it.\n",
        "\n",
        "Let's make our dataset stationary by taking the difference between consecutive elements, which in our case will be the daily change in the number of deaths. To do this, we will use the `diff()` method from the `dataset` DataFrame object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQZT6ps95YtJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(dataset.diff())\n",
        "plt.savefig('daily_deaths_diff', bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un3Egp9z5YtJ"
      },
      "source": [
        "Let's zoom in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B6-gM1E5YtK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "sample = dataset.loc['2020-11-01':'2021-01-01']\n",
        "plt.plot(sample.diff())\n",
        "plt.xticks(sample.index,rotation=90)\n",
        "plt.savefig('daily_deaths_diff_sample', bbox_inches='tight', transparent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6st0Lu-5YtK"
      },
      "source": [
        "Let's count the period length - it's 7 days! The seasonality of deaths is therefore weekly. But there is still an upword trend in the data.\n",
        "\n",
        "Therefore, it is not yet stationary, so let's take the `diff()` one more time. Doing that is what is known as *second-order differencing*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpOD-Jla5YtV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(dataset.diff().diff())\n",
        "plt.savefig('daily_deaths_2nd-order_diff', bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMigw8gY5YtV"
      },
      "source": [
        "This is clearly stationary enough, and we will be using this for our subsequent analysis.\n",
        "\n",
        "The second-order differencing is essentially the difference of the difference, or the change in the change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWMjfaVA5YtV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuSjhKV75YtV"
      },
      "source": [
        "We need to remove the first two data point here, which will be two `None` values after applying `diff()` twice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyUolSPQ5YtV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQkF7k25YtV"
      },
      "source": [
        "The last step in this task is to split our dataset into a training and a test set.\n",
        "- When we train ML models, we typically use 20% of the dataset as a test set.\n",
        "- In time series analysis, the the size of the test set should be close to the extent of the future data.\n",
        "- In this exercise, we are only going to predict 1 month into the future. Therefore, we will use all the data before 15 April 2022 as training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lgCny4q5YtV"
      },
      "outputs": [],
      "source": [
        "cutoff_date = '2022-04-15'\n",
        "\n",
        "print(cutoff_date)\n",
        "\n",
        "train = dataset_diff.loc[dataset_diff.index < pd.to_datetime(cutoff_date)]\n",
        "test = dataset_diff.loc[dataset_diff.index >= pd.to_datetime(cutoff_date)]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(train)\n",
        "plt.savefig('training_set', bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji32Silh5YtW"
      },
      "source": [
        "# Task 3: Forcasting using SARIMAX, or Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6THSKcp5YtW"
      },
      "source": [
        "SARIMAX is one of the time series models in the python statistics library `statsmodels`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC_OWrCc5YtW"
      },
      "source": [
        "SARIMAX uses two sets of parameters:\n",
        "- The `order`: a tuple of values `p`, `q` and `d`. They control the number of parameters in the model. Here, let's use p=2, q=1 and d=3.\n",
        "- The `seasonal_order`: a tuple of values `P`, `D`, `Q` and `s`. They control the seasonal component of the model, and `s` is the periodicity of the dates. So for example, weekly periodicity can be set with `s=7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLSsL4KH5YtW"
      },
      "outputs": [],
      "source": [
        "model = SARIMAX(train, order=(2, 1, 3), seasonal_order=(0,0,0,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L-hNdgb5YtW"
      },
      "source": [
        "Next, we call the fit method to optimize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MmHZT1o5YtW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YDqUZ1B5YtW"
      },
      "source": [
        "Now let's make predictions using the model, and compare those against the values in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HelCWeV5YtW"
      },
      "outputs": [],
      "source": [
        "sarimax_prediction = results.predict(\n",
        "    start=cutoff_date, end='2022-05-15', dynamic=False)\n",
        "plt.figure(figsize=(10, 5))\n",
        "l1, = plt.plot(dataset_diff, label='Observation')\n",
        "l2, = plt.plot(sarimax_prediction, label='ARIMA')\n",
        "plt.legend(handles=[l1, l2])\n",
        "plt.savefig('SARIMAX_prediction', bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsGKiM1h5YtX"
      },
      "source": [
        "Since that we are interested in comparing between the different time series analysis approaches, we are going to use one of the validation measures: mean absolute error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7FxBq685YtX"
      },
      "outputs": [],
      "source": [
        "print('SARIMAX MAE = ', mean_absolute_error(sarimax_prediction, test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxiGKoZi5YtX"
      },
      "source": [
        "However, in a typical machine learning workflow, we should find the best values of p, q and r that will minimize the error. We can use the auto_arima function in the pmdarima module to do that. This will find the optimal parameter combintation and return the best model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bi8CZWe5YtX"
      },
      "outputs": [],
      "source": [
        "model = pm.auto_arima(train, start_p=1, start_q=1,\n",
        "                      m=7,\n",
        "                      d=None,\n",
        "                      seasonal=True,\n",
        "                      start_P=0,\n",
        "                      D=0,\n",
        "                      trace=True,\n",
        "                      error_action='ignore',\n",
        "                      suppress_warnings=True,\n",
        "                      stepwise=True)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxC476FB5YtX"
      },
      "source": [
        "Note that this gives us the best mode here: \"Best model:  SARIMAX(3,0,2)(2,0,1)[7]\". Now let's fit it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zRH8Xca45YtX"
      },
      "outputs": [],
      "source": [
        "sarimax_prediction_atutomated = model.predict(n_periods=test.shape[0])\n",
        "sarimax_prediction_atutomated = pd.DataFrame({'ds':test.index,'y':sarimax_prediction_atutomated})\n",
        "sarimax_prediction_atutomated.set_index('ds',inplace=True)\n",
        "plt.figure(figsize=(10, 5))\n",
        "l1, = plt.plot(dataset_diff, label='Observation')\n",
        "l2, = plt.plot(sarimax_prediction, label='ARIMA')\n",
        "plt.legend(handles=[l1, l2])\n",
        "plt.savefig('SARIMAX_prediction_automated', bbox_inches='tight', transparent=False)\n",
        "print('SARIMAX auto MAE = ', mean_absolute_error(sarimax_prediction_atutomated, test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqkAYlIs5YtX"
      },
      "source": [
        "Woops: the model that is selected by the `auto_arima()` method is doing worse in terms of MAE!\n",
        "\n",
        "This is related to a long discussion on: *which accuracy measure to use when we compare time series models?*\n",
        "\n",
        "We will not have time here to delve into it, so we will just choose the model that has the lowest MAE.\n",
        "\n",
        "Let's run the standard diagnostics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98zb3mMY5YtX"
      },
      "outputs": [],
      "source": [
        "results.plot_diagnostics(figsize=(15, 18))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl1Hh_dW5YtY"
      },
      "source": [
        "- The diagnostic plots enable us to analyse the accuracy of our model, and whether there is any (residual) information in the data that should be used to improve the model.\n",
        "- The package provides four diagnostic plots. Below I only give a very brief description of these plots.\n",
        "  - Standardized residuals over time plot: Calculates the residue, or the difference between observed and predicted values, as a function of time.\n",
        "  - Histogram: It's the count of values against the computed residual. Note: that residual in the x-axis was the one in the y-axis in the plot above.\n",
        "  - Normal Q-Q: Typically, model errors should be normally distributed. This plot checks if this is the case; if the points are nearly linear, then the errors are normally distributed. Which is the case in our plot.\n",
        "  - Correlogram: Checks the autocorrelation in the data, to ensure that the data is random.\n",
        "- *Randomness of data* is a necessary condition for the *validity* of the model, and it is checked by the correlogram.\n",
        "- In the correlogram, the *autocorrelation* is plotted against *time lags*. If the values are close to zero, then this is an indication of the randomness of the data, as is the case in our plot above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4izJ39YJ5YtY"
      },
      "source": [
        "# Task 4: Forcasting using Facebook's Prophet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu2pu0M45YtY"
      },
      "source": [
        "This is the open source time series library released by Facebook. It is also widely used by Facebook in their own time series analysis tasks. Facebook prophet does not require that you specify or search for hyperparameters. The model can act as a black box that does all the required computations on its own. And it works with the same object-fit-predict API.\n",
        "\n",
        "Prophet expects the data frame to have 2 columns, unlike SARIMAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z71BgbAK5YtY"
      },
      "outputs": [],
      "source": [
        "train['ds'] = train.index.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqChQy65YtY"
      },
      "source": [
        "Then we create a new Prophet object and call the `fit()` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0ooFNpy5YtY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goTbg4yo5YtY"
      },
      "source": [
        "- Now let's use the model to generate the predictions for the test set.\n",
        "- First we create the `future` data frame using `make_future_dataframe()`, then we call the `predict()` function.\n",
        "- In `make_future_dataframe()`, we pass the `periods` parameter, which is the number of days we want for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5OidVPY5YtY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhvLV_YD5YtY"
      },
      "source": [
        "Now let us calculate the mean absolute error for our predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOwiz_Vk5YtY"
      },
      "outputs": [],
      "source": [
        "prophet_prediction = prophet_prediction.set_index('ds')\n",
        "prophet_future = prophet_prediction.yhat.loc[prophet_prediction.index >= cutoff_date]\n",
        "print('Prophet MAE = ', mean_absolute_error(prophet_future, test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNDdN35M5YtZ"
      },
      "source": [
        "Next, let's visualize the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIfr7w1l5YtZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "l1, = plt.plot(dataset_diff, label='Observation')\n",
        "l1, = plt.plot(prophet_future, label='Prophet')\n",
        "plt.legend(handles=[l1, l2])\n",
        "plt.savefig('prophet predictions',\n",
        "            bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc_UE8vB5YtZ"
      },
      "source": [
        "# Task 5: Preparing the dataset for XGBOOST and NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5tkCD6I5YtZ"
      },
      "source": [
        "Unlike the prophet and SAIMAX models, the two models we will train in Task 6, namely XGBOOST and NN, are supervised machine learning models that deal with independent data points, or examples. It assumes that each data point is totally independent from the rest of the data points in the dataset.\n",
        "\n",
        "Here is a method that extracts these features from a given dataframe object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX9vq20d5YtZ"
      },
      "outputs": [],
      "source": [
        "def featurize(t):\n",
        "    X = pd.DataFrame()\n",
        "\n",
        "    X['day'] = t.index.day\n",
        "    X['month'] = t.index.month\n",
        "    X['quarter'] = t.index.quarter\n",
        "    X['dayofweek'] = t.index.dayofweek\n",
        "    X['dayofyear'] = t.index.dayofyear\n",
        "    X['weekofyear'] = t.index.weekofyear\n",
        "    y = t.y\n",
        "    return X, y\n",
        "\n",
        "\n",
        "featurize(dataset_diff)[0].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX8FZ09V5YtZ"
      },
      "source": [
        "# Task 6: Create training and test datasets by splitting the dataset, and perform data normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxnusvbG5YtZ"
      },
      "source": [
        "In this task we are going to cover two topics in data preparation: splitting the dataset into training and test data, and normalizing the data.\n",
        "\n",
        "You might have already performed a splitting operation of a dataset in machine learning, where one takes a randomly selected portion of the dataset, say 20%, as a test set, while the remaining 80% is the training set. It is randomly selected because the whole dataset is randomly shufflled before the selection. Another popular approach is the k-fold cross validation.\n",
        "\n",
        "However, those two methods won't work with time series data. The reason is: when we train the model on the training set, the purpose is to predict the target values in the future, which corresponds to date values that are outside of the date values in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpOe3HAv5Yta"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJOUxJBC5Yta"
      },
      "source": [
        "Now let's discuss data normalization. We perform data normalization so as to make the range of values of the features, or the columns in the X_train table, as close as possible. For example, we have the features dayofweek and dayofyear. The range of values of dayofweek is from 1 to 7, whereas dayofyear is from 1 to 365. Having such large differences in the ranges of values will either slow down the training of the machine learning model or make it quite difficult. We solve this problem by applying normalization. There are several ways we can normalize the data with. Here I will choose the StandardScaler, which applies the following equation on each of the columns.\n",
        "\n",
        "z = (x - u) / s\n",
        "\n",
        "Here x is the column before scaling, u is thee mean and s is the standard deviation. So basically, we subtract the mean of each column from itself, then divide by the standard deviation of that column. To apply StandardScaler, we first fit the scaler object to the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf6OVhcY5Yta"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6jgY3Pk5Yta"
      },
      "source": [
        "Apply the scaling to both the training and test sets, as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrJ02a7b5Yta"
      },
      "outputs": [],
      "source": [
        "scaled_train = scaler.transform(X_train)\n",
        "scaled_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seaU-kpf5Yta"
      },
      "source": [
        "# Task 7: Train the XGBOOST and NN models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZJCHYG5Yta"
      },
      "source": [
        "First, create the XGBRegressor object which will represent the XGBOOST regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMYVEHv5Yta"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G69WfGkJ5Yta"
      },
      "source": [
        "Next, train the XGBOOST regression model using the fit method, and perform prediction using the predict method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMLBChjI5Ytb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMvTsa4Y5Ytb"
      },
      "source": [
        "Let us calculate the mean absolute error for the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDvW97ko5Ytb"
      },
      "outputs": [],
      "source": [
        "print('XGBOOST MAE = ', mean_absolute_error(XGBOOST_prediction, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pOXBM6C5Ytb"
      },
      "source": [
        "Creation and training of the feedforward neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d-sQyWV5Ytb"
      },
      "outputs": [],
      "source": [
        "NN_model = Sequential()\n",
        "NN_model.add(Dense(20, input_shape=(scaled_train.shape[1],)))\n",
        "NN_model.add(Dense(10))\n",
        "NN_model.add(Dense(1))\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer=Adam(lr=0.001))\n",
        "NN_model.fit(scaled_train, y_train, validation_data=(\n",
        "    scaled_test, y_test), epochs=210, verbose=1)\n",
        "NN_prediction = NN_model.predict(scaled_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICiSf2R5Ytb"
      },
      "source": [
        "Let's compare the MAE values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO3NCDSr5Ytb"
      },
      "outputs": [],
      "source": [
        "print('XGBOOST MAE = ', mean_absolute_error(XGBOOST_prediction, y_test))\n",
        "print('Prophet MAE = ', mean_absolute_error(prophet_future, test))\n",
        "print('SARIMAX MAE = ', mean_absolute_error(sarimax_prediction, test))\n",
        "print('NN MAE = ', mean_absolute_error(NN_prediction, test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyMCUKOM5Ytb"
      },
      "source": [
        "The peformance of SARIMAX is the lowest, while that of XGBOOST is the highest. <br>\n",
        "Finally let us visualize the predictions of all 4 models. Note the autofmt_xdate method in matplotlib, it knows how to appropriately rotate the date labels on the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLOe8eUQ5Ytb"
      },
      "outputs": [],
      "source": [
        "\n",
        "XGBOOST_df = pd.DataFrame({'y': XGBOOST_prediction.tolist()})\n",
        "XGBOOST_df.index = y_test.index\n",
        "\n",
        "NN_df = pd.DataFrame(NN_prediction)\n",
        "NN_df.index = y_test.index\n",
        "plt.figure(figsize=(20, 20))\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.suptitle('Compare SARIMAX, prophet, XGBOOST and NN')\n",
        "axs[0, 0].plot(dataset_diff.tail(50))\n",
        "axs[0, 0].plot(sarimax_prediction.tail(50))\n",
        "axs[0, 0].set_title(\"SARIMAX\")\n",
        "axs[0, 1].plot(dataset_diff.tail(50))\n",
        "axs[0, 1].plot(prophet_future.tail(50))\n",
        "axs[0, 1].set_title(\"Prophet\")\n",
        "axs[1, 0].plot(dataset_diff.tail(50))\n",
        "axs[1, 0].plot(XGBOOST_df.tail(50))\n",
        "axs[1, 0].set_title(\"XGBOOST\")\n",
        "axs[1, 1].plot(dataset_diff.tail(50))\n",
        "axs[1, 1].plot(NN_df.tail(50))\n",
        "axs[1, 1].set_title(\"NN\")\n",
        "\n",
        "for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "fig.autofmt_xdate()\n",
        "\n",
        "plt.savefig('comparison',\n",
        "            bbox_inches='tight', transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp3jma5G5Ytb"
      },
      "source": [
        "# Task 8: Forecast the future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtr26MmV5Ytc"
      },
      "source": [
        "- We have established the XGBOOST model is the most accurate, and therefore we can use it to forcast the future COVID19 deaths i.e. beyond 15th May 2022.\n",
        "- For the sake of exercise, we will apply all four models for the forecast.\n",
        "- We will ask the four models to forecast 1 month into the future: from 16th May 2022 to 16th June 2022.\n",
        "- Prediction with SARIMAX and fbprophet is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjjB6goY5Ytc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0a1tUmL5Ytc"
      },
      "source": [
        "- For XGBOOST and NN, we need to generate the `X` dataset as we did before. We need to create a new function like `featurize()`, which takes a DataFrame of dates only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cSY6h8s5Ytc"
      },
      "outputs": [],
      "source": [
        "def featurize_dates(t):\n",
        "    X = pd.DataFrame()\n",
        "\n",
        "    X['day'] = t.index.day\n",
        "    X['month'] = t.index.month\n",
        "    X['quarter'] = t.index.quarter\n",
        "    X['dayofweek'] = t.index.dayofweek\n",
        "    X['dayofyear'] = t.index.dayofyear\n",
        "    X['weekofyear'] = t.index.weekofyear\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1MbkBuo5Ytc"
      },
      "source": [
        "Let's apply it on the `future` DataFrame we got from fbprophet to generate forecasts using the trained XGBOOST model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egU4Com95Ytc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0NnoES75Ytc"
      },
      "source": [
        "We do the same for the NN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWZpVxmu5Ytc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0CIet9M5Ytc"
      },
      "source": [
        "Finally, let's plot the entire time series including training, test and future predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZlRbxYK5Ytc"
      },
      "outputs": [],
      "source": [
        "\n",
        "XGBOOST_df = pd.DataFrame({'y': XGBOOST_future.tolist()})\n",
        "XGBOOST_df.index = future.index\n",
        "\n",
        "future_prediction_count = 31 + test.shape[0]\n",
        "\n",
        "NN_df = pd.DataFrame(NN_future)\n",
        "NN_df.index = future.index\n",
        "plt.figure(figsize=(20, 20))\n",
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.suptitle('Compare forecasts of SARIMAX, prophet, XGBOOST and NN')\n",
        "axs[0, 0].plot(dataset_diff.tail(60))\n",
        "axs[0, 0].plot(sarimax_future_prediction.tail(60))\n",
        "axs[0, 0].set_title(\"SARIMAX\")\n",
        "axs[0, 1].plot(dataset_diff.tail(60))\n",
        "axs[0, 1].plot(prophet_future_prediction['yhat'].tail(60))\n",
        "axs[0, 1].set_title(\"Prophet\")\n",
        "axs[1, 0].plot(dataset_diff.tail(60))\n",
        "axs[1, 0].plot(XGBOOST_df.tail(60))\n",
        "axs[1, 0].set_title(\"XGBOOST\")\n",
        "axs[1, 1].plot(dataset_diff.tail(60))\n",
        "axs[1, 1].plot(NN_df.tail(60))\n",
        "axs[1, 1].set_title(\"NN\")\n",
        "\n",
        "for ax in fig.get_axes():\n",
        "    ax.label_outer()\n",
        "fig.autofmt_xdate()\n",
        "\n",
        "plt.savefig('comparison_forecasts',\n",
        "            bbox_inches='tight', transparent=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}